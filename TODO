# Task breakdown

## Backend

[x] Include GTFS files
[x] Open and parse some relevant GTFS table
[x] Show stops, frequencies and travel times along the U8
[x] Create station groups
[x] Show stops and travel times on a route from a point in the middle (including different destinations)
[x] Ignore trips to stops which can be reached faster
[x] Branch travel times from a stop including estimate for transfer times
[x] Start from a parent station
[x] Exclude any stops on trips which wont get somewhere faster in the future
[x] Load routes etc to show better info when plotting the journeys
[x] Calculate bearings for stops
[x] Write algorithm
[x] Produce output json
[x] Search a station by name
[x] Try different start stations
[x] Test loading a larger cache time period
[x] Make a day filter option in the data load
[x] Make web service using old Sunday 7pm data
[x] Build new multi day cache
[x] Station search & selection
[x] Add only stops with a connection to the search
[x] Multi word station search
[x] Bug: failing to transfer to U2 from Alex bhf or from hauptbahnhof tief to upstairs
[x] Error: to go northbound on S2 it's currently suggesting going south to Humbolthain and then changing to north - i guess its not any slower than waiting for that train at humbolthain, need to optimize for less changes
[x] S85 trip is disjointed on earlier stops while it is slow between arrival times and departure times, as these are slow, it should just use departure times
[x] Eberswalderstrasse u2 southbound from sch√∂nhauser allee arrives 30 seconds after the northbound from senefelderplatz, as the transfer takes 3 mins between the platforms they are both the earliest arrivals at their respective stations. We only want to emit the earliest arrival at a station, but for the search we need the earliest arrival at each stop
[x] Filter out transfers to stations with no trips
[x] Time selection
[x] Day filter in search to enable use of multi day cache
[x] Day selection
[x] Handle the D_ stopids in the gtfs data
[x] Get the timetable publish date from the calendar.txt
[x] Filter GTFS data for development so that cache is not needed
[x] Remove the cache
[x] Allow connections to several trips of the same station and route (eg different directions), currently it is filtered to one
[x] Replace arena with a map of trips
[] Station search more forgiving with umlauts etc. maybe find crate to build a linguistic index / search map
[x] Reload GTFS data each day
[] Count + number clients based on IP address + log anonymised when a new client connects
[] Sub count clients by hash of user agent
[] Record logging session per client and log statistical info every minute, every hour and every day + immediate info if enabled by env var
[] Identify regions for the clients by IP address
[] Use exceptions in calendar_dates.txt
[] Handle day overlap
-
[] read from zip
[] CLI tool to lookup departures for debugging
-
[] Find stops within a distance of a point sorted by distance
[] Start from spot between stations
[] Build a graph of average times

# Performance

[x] Improve Time deserialisation, no parsing to string or splitting ~ 25%
[x] Remove duplication of trips read ~ 4%
[x] Parse Time as byte array as checking the char boundaries is slow and unnecessary maybe 6%
[] Parallelize stoptime deserialisation and reading, reading is 30% and deserialisation is 50%
[] Csv reader might be faster with ByteRecord enabling read of borrowed data [https://docs.rs/csv/1.1.3/csv/struct.ByteRecord.html]
[] See if csv reader can be sped up

## Code 
[x] StopId should not be String, maybe &str / str?
[x] Modularise the stoptime reader into a struct by understanding the lifetimes (had to use RefCell)
[x] serialise the data so that i can have faster iteration
[x] Switch to id_arena - it will serialise easier and I won't need all these borrows complicating things
[x] Build and deploy scripts
[] Macro for compile time Time literals
[] Organise all the parsing and data lookups
[] use debugging
[] improve the error reporting in the parser
[] remove the Strings
[] save images and start writing blog post about the development

## Frontend

[x] Use json?
[x] Draw stop points
[x] Connect with lines
[x] Add station names
[x] Use route colours
[x] Add some filtering / highlighting
[x] show wait times / transfers together and distinct from travel
[x] Have the initial wait go to the left to not run over the station name
[x] Draw connections in reverse order to have the first / shorter ones on top
[x] Connect with curves
[x] Trip start control point to reduce curve into the origin
[x] Start bearing toward next stop to avoid curve into the origin
[x] Rewrite in rust/wasm
[] Make it an SVG again - ideally the whole page
[] Hide controls when printing
[] Add key with emphasis highlighting
[] Show station names on hover

# Both

[x] Filters for buses, trams, etc
[x] Shareable Uris
[] Pregenerate the SVG on the backend, add the controls after
[] Start from coords
[] Incoming radar
[] Show only important stations to reduce the number shown
[] Average times
[] London
[] Switch to times from seconds
[] Choose times other than now
[] Smooth animation
  [] hack - how long is first departure from origin - load that much extra data and then just rerender until that departure and then reload data
  [] hack+ - on reload do a complete search in the backend but just send a diff. We may add some new stations and change the earliest arrival time at some other stations. Some trips are removed and added, and some stops on trips which were not shown will be shown. Calculate placement of transfers in frontend
  [] lower the backend calculations by re searching over the existing tree rather than from scratch after a missed departure. This will massively complicate the algorithm and structure and i can't really justify it now.
[] Search up to an hour

# Rust/canvas frontend

[x] Load data in browser
[x] Draw routes
[x] Add frontend controls
[x] Draw grid
[x] Support 2x scaling for retina
[x] Draw connections
[x] Colour routes
[x] Style routes
[x] Redraw without search until a train departs
[x] Route curves
[x] Get data from backend
[x] Pre-search data filtering in backend
[x] Data diffing using session; or
[x] Search from known data while waiting for backend
[] Cleanup - todos, extractions, refactor, hardcoded stuff, edge cases, names, docs, things below
[x] Sync count check and synchronisation and request debounce in client -(with timeout)-
[x] Only animate when the second changes and save cpu
[x] Stations
[x] Figure out scaling again for antialiasing
[x] Implement the draw with geometry as a parameter meaning the view wont have to be recalculated when the time changes (improves performance by having no allocations in the render process - application wont need render loop, it will only need to act on changes and researching)
[x] curved paths not going to the end
[x] Search an extra amount (maximum will be the time to first departure) and then set expiry to that and filter when drawing
[x] Load data before its needed
[x] Draw in proper order
[x] Connections at alex missing
[x] Add station search
[x] Publish wasm version
[x] Share autocomplete component
[x] Fit on a mobile screen
[x] Shareable routing
[x] Colour properly
[] Use stroke dashes and stroke width from csv too
[] Make sure it doesn't animate when not visible to save cpu : based on FF task manager, it uses barely any cpu when off screen
[] New name (not colliding with translate.google.com) and description of what it does
[x] Show time and timetable time
[] Is there an analogy from quantum or analogue computing?
[] Debug logging
[x] dotted lines are going to the second stop on a trip from the origin
[] Preselect checkboxes
---
[] Reduce size of wasm
[] Break up large loads into smaller parts to show something quicker and to avoid a long block while parsing
[] more transfer efficient way of indexing / syncing the data
[] Presearch stations in local data - show those results at the top
[] WS
---
[] Fade in/out
[] Walking transfer stations should not get closer
[x] Animate search change
[] Geographical mode
[] Change time
[] Use location and have initial walk to stations
---
[] Debounce needs a timeout - or does it?
[] Initial curve for S8 / S1
[] Fast mode
[] Write route name at end of route
[] Use RCs to avoid copying
[] Move data sync / search back to the seed app, canvas view only needs the search results?
[] Decent time sync between front and back (backend responsible for macro time and frontend for micro - effectively just an offset form the frontend time)
[] Get time initially from backend
[] Don't freeze display thread while deserialising
[] Fast load with first image from backend (& no script)
[] Click station to show from there (from arrival time or now?)
[] Animate with video stream form backend

# Models

1. GTFS data
2. Indexed data for searching
3. Search algorithm temporary
4. Search output
5. Frontend model transferred to client
6. SVG model

## New frontend

To redesign the frontend for lower bandwidth, smooth animation, lower cpu usage, the potential for backend performance improvements and interactivity on the frontend.

5. Filtered model of just what the frontend needs transferred to client.
  This would have a reduced amount of duplicated data, possibly by have a stateful session so the backend knows what the client already has, or better if the client could represent it's existing state to the backend (it's better for scaling but it probably wont manage the performance).
  Taking lessons from graphql, the client should be in charge of what info it needs
6. Frontend data model (possibly also stored on backend to diff against) the model may be the same as the backend search data, just filtered to what is relevant
7. Do we then run the search again on the frontend data that has been synced? Can it use the same implementation?
8. Search result to display
9. Display model which can be updated interactively and animate transitions without searching again. This model should be fast to draw and run on the render thread (60fps hopefully)

Changes to do for this:
[x] Make the search model and algorithm able to be used on the frontend as well - not depending on GTFS stuff, serializable, extracted to it's own crate etc.
[x] Use search algorithm on backend to produce filtered search data to be used by the same algorithm.
[x] Use search algorithm on frontend
[x] Build a 2 stage renderer 1 converts the search result into a sort of DOM and accepts display parameters (and can be switched) and another that renders the DOM to canvas, and implements transitions
[x] Diff filtered data on backend and sync to frontend


## Memory usage reduction (which may speed up startup too)

2022-02-20

Currently, when the backend starts up, it is using about half a gig of ram, there is a lot of data to store (the gtfs dir is 477MB) but it could surely be used more efficiently and maybe save 100/200MB and make the start up quicker.
Activity monitor reports 526MB used, real memory 578MB, Virtual 34GB Private 525MB.

DHAT profiling with `time cargo run --features dhat-heap --release` reports that after loading the data, 179MB is allocated for `radar-search/src/search_data.rs:556:18` which is `departures.entry(departure_time).or_default().push(stop_ref)` and 126MB is allocated in `radar-search/src/search_data.rs:538:9` which is `trip.stop_times.push(StopTime {`. These are both pushing to vecs. My first thought is that it's likely that these Vecs have spare capacity at the end, so we could free the unused parts once the load is done. But that freeing would likely leave very fragmented memory  (the first ends up with 2.5 Million blocks allocated) so it might not be useful, so it might make sense to find another way to store this data without using dynamically-growing vecs, this might also speed up the load. Also, I might be able to find a more compact representation.

```
5524942 departures of 239248 trips, leaving from 41833 stops
built station name index of 9448 words
Starting web server on port 8080
dhat: Total:     718,419,242 bytes in 4,918,433 blocks
dhat: At t-gmax: 521,567,452 bytes in 4,065,985 blocks
dhat: At t-end:  521,567,452 bytes in 4,065,985 blocks

real    0m21.349s
```

Running `shrink_to_fit` on both at the end of the load results in activity monitor reporting less virtual memory used but more real memory. DHAT reports 30M less allocated at the end.
```5524942 departures of 239248 trips, leaving from 41833 stops
built station name index of 9448 words
Starting web server on port 8080
dhat: Total:     875,689,898 bytes in 7,568,659 blocks
dhat: At t-gmax: 507,960,913 bytes in 3,837,773 blocks
dhat: At t-end:  392,525,148 bytes in 4,065,985 blocks
dhat: The data has been saved to dhat-heap-setup.json, and is viewable with dhat/dh_view.html

real    0m29.830s
```

I saved 50MB first by halving the size of the `TripStopRef`.

It would be nice to save some of the memory fragmentation by storing all the stoptimes contiguously in memory and then splitting them up between the trips. But these types also need to handle being serialized to the client, so it is a little tricky.

Going beyond these savings would most likely need to involve not loading everything into memory.

## Return to SVG frontend

At a high level, I want the backend returning SVG, so the document would have media type image/svg+xml, I can probably then use a combination of SMIL animation and JS (with additional SMIL animations loaded over websockets or webtransport) to achieve the live animation.

SVG will allow:

* Saving the image
* Vector graphics
* Dark/Light theme
* Using browser optimisation for animation should improve the performance

1. [x] Generate a static departure graph SVG on the backend
 - [x] Better SVG drawing macros
 - [x] Probably use CSS for all the styling
 - [x] hidden paths
 - [x] fix the incorrect bezier curve
 - [x] no gaps in routes
 - [x] transfer on later trip join doesn't connect properly
 - [x] Reduce prominence of sibling stations (ie. so that Alexanderplatz isn't a total mess) perhaps just show a small dot and no name, or perhaps no dot
 - [ ] Replace station suggestion somehow
 - [x] Parsing errors
 - [x] Add automatic reload to achieve animation
 - [x] Bus error from Wannsee
 - [x] Ferries
 - [x] Issue with successive bus stops arriving in the same minute : http://127.0.0.1:8000/depart-from/900000110004/2022-03-28T10:45:15?minutes=4&mode=bus
 - [x] line not curving when it's only one segment : http://127.0.0.1:8000/depart-from/7157/2022-04-01T17:54:54.876?minutes=20&mode=sbahn,boat,regional,bus,tram,ubahn
 - [ ] Memory and time performance benchmark
 - [ ] Try removing whole trip enqueuing and filtering
 - [ ] Make ready to deploy
 - [ ] Default colour for trams
2. Enhance
 - [x] Style the image for dark mode
 - [x] Click on other stations to change origin
 - [x] Choose a time or now
 - [x] Choose duration
 - [x] Header
 - [x] Clickable filters
 - [ ] Choose size or scale
 - [ ] On hover:
  - [ ] Make it bigger and bring to the front
  - [ ] Make all the trips and interchange stations on the way to it larger and bring to front
 - [ ] On click:
  - [ ] Show details about the arrival at the station or about the trip and it's stop times
  - [ ] Show details of the previous steps to get to that point de-emphasised
  - [ ] Link to show departures from on stations
 - [ ] Make sure it saves nice
  - [ ] compress links as there are a lot of them on an svg
 - [ ] If 2 different lines arrive at a stop at the same time (eg. Mehringdamm) maybe we should show both
 - [ ] Scoring for how easy the connection to an added trip or station was, this can be used to decide whether to make a new connection to a trip at a later point. Later this can be used for prefereneces about less transfers / less walking / les waiting / speed
3. Expand
 - [ ] Use calendar and calendar dates
 - [ ] Coordinate start points
 - [ ] Station Arrival graph (Cause & effect)
 - [ ] On-train graph
 - [ ] On-train arrival graph
 - [ ] Good navigation
 - [ ] Add some other cities
 - [ ] render iso bars as times instead of durations
 - [ ] departures from 2 different places on one chart, with an intersecting line showing where the distance is the same. It would only show the stations that both can reach within 30 minutes, the positioning choose between 2 points which has the correct isochronic distance from both origins
 - [ ] Different options for deciding the best route (less transfers / less walking / les waiting / speed)
 - [ ] Switch to regular map
4. Live view
 - [ ] Animate the passage of time on all views when looking at now
 - [ ] Animate switching between views and start points and filters
 - [ ] Live departure updates
 - [ ] sample allocations per request

## Data structure

There are 2 big problems with the way the data works in the app, one is that it takes a long time to load up the data, and the other is that it uses a lot of memory (>500MB). Both problems are caused by having to read the whole of the gtfs data on each load up and keep it in memory the whole time. The effect is bad DX and costly production, so I'd like to fix it.

I think a lot of the load time is actually taken by allocations, so a big saving could be made by avoiding allocations, but this saving is limited, to make the load really fast, the data needs to be preprocessed and stored on-disk, so it can be loaded and used without parsing everything and building new structures every time.

To also reduce the amount of memory used, we need to avoid loading data until it will actually be used, only 10's of MBs should be needed to answer a query, we might also want to remove things from memory which are no longer used. This means processing the GTFS data and producing a different format of data which the search algorithm can randomly read parts of. In order to avoid seeks and reads all over the place and keep the search fast, we need to keep data that will be used together clustered in ways that localises data that will be used together.

### Localising data

The station, trip, route, etc. data is not so huge, and so we may well load all of it, but the stop times are huge and that's where we need the localisation of data so that only a small amount is loaded. The simplest localisation of stop times needed in a query is by time, for a 30min radar, we only need stop times which depart and arrive during that 30mins. I imagine that this reduces the size of stoptimes needed in memory by around 30x. The main lookup is by station though and this needs to be fast, so we can't search through every departure in 30 mins looking for ones from a particular station.

I think the solution would be to partition time into blocks of around 10/15 mins to be stored together in memory, within each block there would then be a table keyed on the stopid, pointing to an ordered table of the departures from the stop.

```
MSBs of Time -> StopId -> LSBs of Time* -> (trip_id, arrival_time, departure_time, previous_stop_id, next_stop_id)
```

Only a few of these blocks would be needed for a query.
Maybe the LSB's of time don't need to be indexed as most stops will only have a few departures per block.
If the arrival time and departure time are in different blocks, do we need to duplicate them?
The stop_id lookup is probably the trickiest part, there could be over 10,000 stops with departures in a block and we need to look up every stop we come across in the search (in several blocks).

### Building data

This datastore needs to be rebuilt each time a new gtfs archive is fetched. Thankfully it can actually be done while an existing version is using the old store, but if it uses 500MB to build then it doesn't actually reduce the peak memory usage in production, it just reduces that to only being during the data build. Ideally we would avoid having to load all the data into memory in order to build this store, but we probably need to load all the stop times in order to do it properly.

### Parsing or MMapping

When the search algo finds blocks it needs for searching, we could either then seek to that block and deserialise it, or if we use mmap, we could let the OS deal with reading pages, but that means we need only mmapable structures, which means we can't use BTreeMap, etc. If we do want to use BTreeMap for the stop_id lookup, we need to parse a loopu table and load it into a BTree, this could either be done for each block when it is loaded, or there could be a single lookup for all of them, which would be built at startup and might be 10MB

### Todo

- [ ] Remove the trip stoptime storage, I think it was only needed for the sync, and maybe not even
- [ ] See if https://docs.rs/flatdata/latest/flatdata/ (or something similar would work for this storage
- [ ] Otherwise look into mmapable indexes / maps / lookups
- [ ] Binary which:
 - [x] reads all the stop times into memory - 33secs
 - [ ] Puts previous_stop and next_stop on each
 - [ ] creates indexes of both time fields
 - [ ] checks the page size and calculates maximum number of stop times per page
 - [ ] buckets the stoptimes into pages, writing them into a file
 - [ ] records a table of content to the file
 - [ ] reads the file back and verifies it
